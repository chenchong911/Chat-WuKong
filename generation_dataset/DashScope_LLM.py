# /home/zlx/chat-wukong/generation_dataset/DashScope_LLM.py

import dashscope
import os
import time
import random
from langchain.llms.base import LLM
from typing import Dict, List, Optional, Any
from dotenv import find_dotenv, load_dotenv

_ = load_dotenv(find_dotenv())

# é˜¿é‡Œäº‘ç™¾ç‚¼APIé…ç½®
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")

def get_completion(prompt: str, 
                   model: str = "qwen-plus", 
                   temperature: float = 0.7, 
                   max_retries: int = 5,
                   top_p: float = 0.8,
                   max_tokens: int = 2048) -> str:
    '''
    è°ƒç”¨DashScope APIè·å–æ–‡æœ¬ç”Ÿæˆç»“æœ
    
    Args:
        prompt: è¾“å…¥æç¤ºè¯
        model: è°ƒç”¨çš„æ¨¡å‹ï¼Œå¯é€‰: qwen-turbo, qwen-plus, qwen-max
        temperature: æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ (0-2.0)
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        top_p: nucleus samplingå‚æ•° (0-1.0)
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
    
    Returns:
        str: ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
    '''
    if not DASHSCOPE_API_KEY:
        raise ValueError("æœªè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    
    # è®¾ç½®APIå¯†é’¥
    dashscope.api_key = DASHSCOPE_API_KEY
    
    for attempt in range(max_retries):
        try:
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œé¿å…é¢‘ç‡é™åˆ¶
            if attempt > 0:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"ğŸ”„ ç¬¬{attempt+1}æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
            
            # è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API
            response = dashscope.Generation.call(
                model=model,
                prompt=prompt,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                repetition_penalty=1.1,  # å‡å°‘é‡å¤
                seed=42  # å›ºå®šç§å­ä»¥è·å¾—ä¸€è‡´ç»“æœ
            )
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code == 200:
                return response.output.text
            else:
                error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}"
                if attempt < max_retries - 1:
                    print(f"âš ï¸  {error_msg}ï¼Œå‡†å¤‡é‡è¯•...")
                    continue
                else:
                    raise Exception(error_msg)
                    
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {str(e)}ï¼Œå‡†å¤‡é‡è¯•...")
                continue
            else:
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")

class DashScope_LLM(LLM):
    """é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°çš„å¤§è¯­è¨€æ¨¡å‹æ¥å£"""
    
    model_name: str = "qwen-turbo"  # é»˜è®¤ä½¿ç”¨qwen-turboï¼Œé€Ÿåº¦å¿«ä¸”ç¨³å®š
    temperature: float = 0.7
    top_p: float = 0.8
    max_tokens: int = 2048
    max_retries: int = 5

    def __init__(self, 
                 model_name: str = "qwen-turbo", 
                 temperature: float = 0.7,
                 top_p: float = 0.8,
                 max_tokens: int = 2048,
                 max_retries: int = 5):
        super().__init__()
        self.model_name = model_name
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.max_retries = max_retries

    @property
    def _llm_type(self) -> str:
        return "dashscope"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:
        """
        è°ƒç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            stop: åœæ­¢è¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            res = get_completion(
                prompt=prompt,
                model=self.model_name,
                temperature=self.temperature,
                max_retries=self.max_retries,
                top_p=self.top_p,
                max_tokens=self.max_tokens
            )
            return res
        except Exception as e:
            raise Exception(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è·å–æ ‡è¯†å‚æ•°"""
        return {
            "model_name": self.model_name,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "max_tokens": self.max_tokens,
            "max_retries": self.max_retries
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    llm = DashScope_LLM(
        model_name="qwen-plus",
        temperature=0.7,
        max_tokens=1024
    )
    
    # æµ‹è¯•è°ƒç”¨
    try:
        result = llm("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
        print("ç”Ÿæˆç»“æœ:", result)
    except Exception as e:
        print(f"è°ƒç”¨å¤±è´¥: {e}")